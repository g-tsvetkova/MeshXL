Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 313, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 298, in main
    accelerator.prepare(
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1339, in prepare
    result = tuple(
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1340, in <genexpr>
    self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1215, in _prepare_one
    return self.prepare_model(obj, device_placement=device_placement)
  File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1469, in prepare_model
    model = torch.nn.parallel.DistributedDataParallel(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
    _verify_param_shape_across_processes(self.process_group, parameters)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
    return dist._verify_params_across_processes(process_group, tensors, logger)
torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:317, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
Last error:
Duplicate GPU detected : rank 0 and rank 4 both on CUDA device 1000
[rank0]: Traceback (most recent call last):
[rank0]:   File "/root/MeshXL/main.py", line 313, in <module>
[rank0]:     main(args)
[rank0]:   File "/root/MeshXL/main.py", line 298, in main
[rank0]:     accelerator.prepare(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1339, in prepare
[rank0]:     result = tuple(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1340, in <genexpr>
[rank0]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1215, in _prepare_one
[rank0]:     return self.prepare_model(obj, device_placement=device_placement)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/accelerate/accelerator.py", line 1469, in prepare_model
[rank0]:     model = torch.nn.parallel.DistributedDataParallel(
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/root/.multi3d/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:317, invalid usage (run with NCCL_DEBUG=WARN for details), NCCL version 2.21.5
[rank0]: ncclInvalidUsage: This usually reflects invalid usage of NCCL library.
[rank0]: Last error:
[rank0]: Duplicate GPU detected : rank 0 and rank 4 both on CUDA device 1000
