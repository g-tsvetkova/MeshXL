Loading train data with 515 files.
Loading test data with 129 files.
	[train]	transformer.model.decoder.embed_tokens.weight:	torch.Size([131, 768])
	[train]	transformer.model.decoder.embed_positions.weight:	torch.Size([8194, 768])
	[train]	transformer.model.decoder.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.0.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.0.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.0.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.0.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.0.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.1.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.1.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.1.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.1.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.1.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.2.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.2.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.2.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.2.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.2.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.3.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.3.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.3.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.3.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.3.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.4.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.4.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.4.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.4.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.4.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.5.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.5.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.5.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.5.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.5.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.6.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.6.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.6.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.6.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.6.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.7.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.7.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.7.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.7.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.7.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.8.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.8.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.8.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.8.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.8.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.9.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.9.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.9.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.9.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.9.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.10.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.10.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.10.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.10.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.10.final_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.k_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.v_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.q_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.weight:	torch.Size([768, 768])
	[train]	transformer.model.decoder.layers.11.self_attn.out_proj.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.self_attn_layer_norm.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.fc1.weight:	torch.Size([3072, 768])
	[train]	transformer.model.decoder.layers.11.fc1.bias:	torch.Size([3072])
	[train]	transformer.model.decoder.layers.11.fc2.weight:	torch.Size([768, 3072])
	[train]	transformer.model.decoder.layers.11.fc2.bias:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.weight:	torch.Size([768])
	[train]	transformer.model.decoder.layers.11.final_layer_norm.bias:	torch.Size([768])
call with args: Namespace(base_lr=0.0001, final_lr=1e-06, weight_decay=0.1, clip_gradient=0.1, warm_lr=1e-06, warm_lr_iters=1000, pad_id=-1, dataset='objaverse', augment=False, n_discrete_size=128, n_max_triangles=800, model='mesh_xl', llm='mesh-xl/mesh-xl-125m', text_condition=None, image_condition=None, pretrained_weights=None, dataset_num_workers=0, batchsize_per_gpu=4, start_epoch=0, max_epoch=16, start_eval_after=-1, eval_every_iteration=4000, seed=0, test_only=False, sample_rounds=100, criterion=None, test_ckpt='', checkpoint_dir='./checkpoints', save_every=20000, log_every=10)
MeshXL(
  (tokenizer): MeshTokenizer()
  (transformer): OPTForCausalLM(
    (model): OPTModel(
      (decoder): OPTDecoder(
        (embed_tokens): Embedding(131, 768, padding_idx=130)
        (embed_positions): OPTLearnedPositionalEmbedding(8194, 768)
        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (layers): ModuleList(
          (0-11): 12 x OPTDecoderLayer(
            (self_attn): OPTAttention(
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (activation_fn): ReLU()
            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          )
        )
      )
    )
    (lm_head): Linear(in_features=768, out_features=131, bias=False)
  )
)
<accelerate.data_loader.DataLoaderShard object at 0x7ff81e2c4520>
Raw vertices:  tensor([[[ 0.4481,  0.1069, -0.2154],
         [ 0.4481,  0.1069, -0.2154],
         [ 0.4481,  0.1069, -0.2154],
         ...,
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000]],

        [[-6.5338,  0.3659, -1.6204],
         [-6.5338,  0.3659, -1.6204],
         [-6.5338,  0.3659, -1.6204],
         ...,
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000]],

        [[-0.0912, -0.3667, -0.1373],
         [-0.0912, -0.3667, -0.1373],
         [-0.0912, -0.3667, -0.1373],
         ...,
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000]],

        [[ 0.1986,  0.0144,  0.0376],
         [ 0.4318,  0.0158,  0.0376],
         [ 0.3169,  0.1334,  0.0376],
         ...,
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.0000,  0.0000,  0.0000]]], device='cuda:0')
Raw faces:  tensor([[[ 198,   16,  138],
         [ 138,   16,   19],
         [ 138,   19,  202],
         ...,
         [  -1,   -1,   -1],
         [  -1,   -1,   -1],
         [  -1,   -1,   -1]],

        [[  50,   60,    6],
         [  50,    6,    0],
         [  51,    1,    3],
         ...,
         [  -1,   -1,   -1],
         [  -1,   -1,   -1],
         [  -1,   -1,   -1]],

        [[ 909, 1020,  945],
         [ 945, 1020, 1014],
         [ 945, 1014,  561],
         ...,
         [  -1,   -1,   -1],
         [  -1,   -1,   -1],
         [  -1,   -1,   -1]],

        [[ 171,  191,  195],
         [ 151,  171,  195],
         [ 139,  151,  195],
         ...,
         [  -1,   -1,   -1],
         [  -1,   -1,   -1],
         [  -1,   -1,   -1]]], device='cuda:0')
Face mask:  tensor([[ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False],
        [ True,  True,  True,  ..., False, False, False]], device='cuda:0')
torch.Size([4, 800, 2500, 3])
torch.Size([4, 800, 3, 3])
max face index: 1239 num_vertices: 800
min face index: 0
Discrete face coordinates:  tensor([[[[79, 78, 50],
          [78, 78, 50],
          [79, 79, 50]],

         [[79, 79, 50],
          [78, 78, 50],
          [78, 80, 50]],

         [[79, 79, 50],
          [78, 80, 50],
          [79, 80, 50]],

         ...,

         [[92, 70, 50],
          [92, 70, 50],
          [92, 70, 50]],

         [[92, 70, 50],
          [92, 70, 50],
          [92, 70, 50]],

         [[92, 70, 50],
          [92, 70, 50],
          [92, 70, 50]]],


        [[[ 0, 87,  0],
          [ 0, 90,  0],
          [ 0, 90,  0]],

         [[ 0, 87,  0],
          [ 0, 90,  0],
          [ 0, 87,  0]],

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]],

         ...,

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]],

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]],

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]]],


        [[[71, 67, 71],
          [71, 67, 71],
          [71, 67, 71]],

         [[71, 67, 71],
          [71, 67, 71],
          [71, 67, 71]],

         [[71, 67, 71],
          [71, 67, 71],
          [68, 67, 68]],

         ...,

         [[58, 40, 55],
          [58, 40, 55],
          [58, 40, 55]],

         [[58, 40, 55],
          [58, 40, 55],
          [58, 40, 55]],

         [[58, 40, 55],
          [58, 40, 55],
          [58, 40, 55]]],


        [[[65, 64, 71],
          [65, 64, 72],
          [63, 64, 73]],

         [[64, 64, 70],
          [65, 64, 71],
          [63, 64, 73]],

         [[64, 64, 70],
          [64, 64, 70],
          [63, 64, 73]],

         ...,

         [[76, 64, 66],
          [76, 64, 66],
          [76, 64, 66]],

         [[76, 64, 66],
          [76, 64, 66],
          [76, 64, 66]],

         [[76, 64, 66],
          [76, 64, 66],
          [76, 64, 66]]]], device='cuda:0')
Discrete face coordinates:  tensor([[[[79, 78, 50],
          [78, 78, 50],
          [79, 79, 50]],

         [[79, 79, 50],
          [78, 78, 50],
          [78, 80, 50]],

         [[79, 79, 50],
          [78, 80, 50],
          [79, 80, 50]],

         ...,

         [[92, 70, 50],
          [92, 70, 50],
          [92, 70, 50]],

         [[92, 70, 50],
          [92, 70, 50],
          [92, 70, 50]],

         [[92, 70, 50],
          [92, 70, 50],
          [92, 70, 50]]],


        [[[ 0, 87,  0],
          [ 0, 90,  0],
          [ 0, 90,  0]],

         [[ 0, 87,  0],
          [ 0, 90,  0],
          [ 0, 87,  0]],

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]],

         ...,

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]],

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]],

         [[ 0, 87,  0],
          [ 0, 87,  0],
          [ 0, 87,  0]]],


        [[[71, 67, 71],
          [71, 67, 71],
          [71, 67, 71]],

         [[71, 67, 71],
          [71, 67, 71],
          [71, 67, 71]],

         [[71, 67, 71],
          [71, 67, 71],
          [68, 67, 68]],

         ...,

         [[58, 40, 55],
          [58, 40, 55],
          [58, 40, 55]],

         [[58, 40, 55],
          [58, 40, 55],
          [58, 40, 55]],

         [[58, 40, 55],
          [58, 40, 55],
          [58, 40, 55]]],


        [[[65, 64, 71],
          [65, 64, 72],
          [63, 64, 73]],

         [[64, 64, 70],
          [65, 64, 71],
          [63, 64, 73]],

         [[64, 64, 70],
          [64, 64, 70],
          [63, 64, 73]],

         ...,

         [[76, 64, 66],
          [76, 64, 66],
          [76, 64, 66]],

         [[76, 64, 66],
          [76, 64, 66],
          [76, 64, 66]],

         [[76, 64, 66],
          [76, 64, 66],
          [76, 64, 66]]]], device='cuda:0')
Traceback (most recent call last):
  File "/root/MeshXL/main.py", line 313, in <module>
    main(args)
  File "/root/MeshXL/main.py", line 303, in main
    do_train(
  File "/root/MeshXL/engine.py", line 64, in do_train
    outputs = model(batch_data_label)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/MeshXL/models/mesh_xl/get_model.py", line 272, in forward
    return self.train_one_step(data_dict)
  File "/root/MeshXL/models/mesh_xl/get_model.py", line 315, in train_one_step
    output = self.transformer(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 1145, in forward
    outputs = self.model.decoder(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 911, in forward
    layer_outputs = decoder_layer(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 552, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.multi3d/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 242, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1)
  File "/root/.multi3d/lib/python3.10/site-packages/torch/nn/functional.py", line 2140, in softmax
    ret = input.softmax(dim)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 9.28 GiB. GPU 0 has a total capacity of 23.68 GiB of which 2.43 GiB is free. Process 1324117 has 21.25 GiB memory in use. Of the allocated memory 20.25 GiB is allocated by PyTorch, and 714.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
